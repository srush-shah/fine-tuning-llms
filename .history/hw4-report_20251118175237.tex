\documentclass{article}

\input{header}

\title{DSGA 1011: Assignment 4}

\author{Full Name \\ Net ID}

\date{}


\colmfinalcopy
\begin{document}
\maketitle
% \section*{Part I. Q1} No written element, submit \texttt{out\_original.txt}  to autograder.
\section*{Q0. 1.}
Please provide a link to your github repository, which contains the code for both Part I and Part II. \textcolor{gray}{TODO}
\section*{Q2. 1.}
Describe your transformation of dataset.

The data transformation applies two types of augmentations to text: synonym replacement and typo simulation. 

\textbf{Synonym Replacement:} For each word in the text, with 20\% probability, we attempt to replace it with a synonym from WordNet. We extract synonyms from all synsets for the word, filter out synonyms containing underscores (to avoid multi-word phrases), and randomly select one. The original capitalization is preserved.

\textbf{Typo Simulation:} For words that were not replaced by synonyms, with 15\% probability, we simulate keyboard typos by replacing a character with a neighboring key on the QWERTY keyboard. We focus on vowels and common letters (a, e, i, o, u, s, d, f, g, h, j, k, l) and replace at most one character per word with a randomly selected neighbor. The original capitalization is preserved.

Both transformations are applied at the word level after tokenization, and the text is detokenized to produce the final transformed example. This approach introduces natural variations that simulate real-world text noise while preserving the semantic meaning of the original text.
% \section*{Part I. Q2. 2. No written element, submit \texttt{out\_transformed.txt} to autograder. }
\section*{Q3. 1}
\textbf{Report \& Analysis}
    \begin{itemize}
        \item \textbf{Accuracy values:}
        \begin{itemize}
            \item Model trained on original data, evaluated on original test: 93.12\%
            \item Model trained on original data, evaluated on transformed test: 88.70\%
            \item Model trained on augmented data, evaluated on original test: 93.04\%
            \item Model trained on augmented data, evaluated on transformed test: 90.81\%
        \end{itemize}
        
        \item \textbf{Analysis:}
        \begin{enumerate}
            \item \textit{Did the model's performance on the transformed test data improve after applying data augmentation?} Yes, the accuracy on transformed test data improved from 88.70\% (without augmentation) to 90.81\% (with augmentation), an improvement of 2.11 percentage points. This indicates that data augmentation helped the model generalize better to the transformed (noisy) test distribution.
            
            \item \textit{How did data augmentation affect the model's performance on the original test data?} Data augmentation slightly decreased performance on the original test data from 93.12\% to 93.04\%, a small drop of 0.08 percentage points. This suggests that while augmentation helps with robustness to transformations, it may introduce a small trade-off in performance on the original distribution, though the difference is minimal and likely within statistical variance.
        \end{enumerate}
        
        \item \textbf{Intuitive explanation:} Data augmentation exposes the model to more diverse training examples that include synonym variations and typos similar to those in the transformed test set. This helps the model learn more robust features that are invariant to these types of noise. The model learns to focus on semantic content rather than exact word matches, which improves generalization to the transformed test set. The slight decrease on original test data may occur because the augmented training set includes some noisy examples that slightly shift the decision boundary, but the effect is minimal since the majority of training data remains clean.
        
        \item \textbf{Limitation:} One key limitation is that the augmentation only covers specific types of noise (synonyms and keyboard typos) that are similar to the test-time transformations. For truly out-of-distribution test sets with different types of noise (e.g., grammatical errors, domain shifts, or completely different writing styles), this augmentation approach may not help. The model may overfit to the specific augmentation patterns seen during training and fail to generalize to other types of distribution shifts. Additionally, the augmentation probabilities (20\% for synonyms, 15\% for typos) are fixed and may not reflect the actual distribution of noise in real-world scenarios.
    \end{itemize}
\section*{Part II. Q4}
% 
% \section{Data Statistics and Processing (8pt)}


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
Number of examples & 4225 & 466 \\
Mean sentence length & 10.96 & 10.91 \\
Mean SQL query length & 60.90 & 58.90  \\
Vocabulary size (natural language)& 868 & 444  \\
Vocabulary size (SQL)& 533 & 326  \\
\bottomrule
\end{tabular}
\caption{Data statistics before any pre-processing. \textcolor{gray}{You need to at least provide the statistics listed above, and can add new entries.}}
\label{tab:data_stats_before}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
\multicolumn{3}{l}{\textbf{T5 fine-tuned model}} \\ % \textcolor{gray}{(T5 fine-tuning or T5 from scratch)}} \\
Mean sentence length (tokens) & 10.96 & 10.91 \\
Mean SQL query length (tokens) & 60.90 & 58.90  \\
Vocabulary size (natural language) & 868 & 444  \\
Vocabulary size (SQL) & 533 & 326  \\
\midrule
\bottomrule
\end{tabular}
\caption{Data statistics after pre-processing. After tokenization with T5TokenizerFast, the statistics remain similar to the raw data since T5 tokenization preserves most word boundaries. The vocabulary sizes reflect the unique tokens in the tokenizer's vocabulary that appear in the dataset.}
\label{tab:data_stats_after}
\end{table}



\newpage




\section*{Q5}\label{sec:t5}


\begin{table}[h!]
\centering
\begin{tabular}{p{3.5cm}p{10cm}}
\toprule
Design choice & Description \\
\midrule
Data processing & Added task prefix ``translate English to SQL: '' to all natural language queries. No other preprocessing was applied to the SQL queries or natural language inputs. \\
Tokenization & Used T5TokenizerFast from 'google-t5/t5-small' for both encoder and decoder. Encoder inputs were tokenized with the task prefix included. Decoder targets (SQL queries) were tokenized with add\_special\_tokens=True to include EOS tokens. Maximum sequence lengths: 256 tokens for both encoder and decoder inputs. \\
Architecture & Fine-tuned the entire T5-small model (all encoder and decoder layers). The model was initialized from the pretrained 'google-t5/t5-small' checkpoint. \\
Hyperparameters & Learning rate: 1e-4, Batch size: 32, Optimizer: AdamW, Scheduler: Cosine with 1 warmup epoch, Max epochs: 20, Patience: 3 epochs (early stopping based on dev F1 score), Weight decay: 0.0. Generation: Greedy decoding (num\_beams=1), max\_new\_tokens=256, repetition\_penalty=1.2. \\
\bottomrule
\end{tabular}
\caption{Details of the best-performing T5 model configurations (fine-tuned)}
\label{tab:t5_results_ft}
\end{table}







\section*{Q6. }

\paragraph{Quantitative Results:} 
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
  \toprule
  System & Query EM & F1 score\\
  \midrule
  \multicolumn{3}{l}{\textbf{Dev Results}} \\
  \midrule
  
  \multicolumn{3}{l}{\textbf{T5 fine-tuned}} \\
  Full model & 0.00\% & 11.80\% \\[5pt]
  % Variant1 & XX.XX & XX.XX \\
  % Variant2 & XX.XX & XX.XX \\
  % Variant3 & XX.XX & XX.XX \\
  
  \midrule
  \multicolumn{3}{l}{\textbf{Test Results}} \\
  \midrule
  T5 fine-tuning & 0.00\% & 11.80\% \\
  \bottomrule
\end{tabular}  
\caption{Development and test results. \textcolor{gray}{Use this table to report quantitative results for both dev and test results.}}
\label{tab:results}
\end{table}


\paragraph{Qualitative Error Analysis:} 


\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{p{2cm}p{6cm}p{6cm}p{6cm}}
    \toprule
    \textbf{Error Type}& \textbf{Example Of Error} & \textbf{Error Description} & \textbf{Statistics} \\
    \midrule
    Repetitive token generation & \texttt{SELECT DISTINCT flight\_service\_service\_service\_service...} & The model generates repetitive sequences of the same tokens (e.g., ``service\_service\_service'' or ``airport\_code = airport\_service\_1.airport\_code = ...''), indicating the model gets stuck in generation loops. & 466/466 (100\% of queries show some form of repetition) \\
    \midrule
    Malformed SQL syntax & \texttt{SELECT DISTINCT flight\_1.airport\_code = airport\_service\_1.airport\_code = ...} & Generated queries contain invalid SQL syntax, such as multiple consecutive equals signs, missing WHERE clauses, or incorrect table/column references. & 466/466 (100\% SQL error rate) \\
    \midrule
    Incomplete queries & \texttt{denver nach atlanta} & Some queries are completely malformed, generating natural language text instead of SQL, or producing queries that are missing critical SQL components. & ~50/466 (approximately 10-15\% of queries) \\
    \midrule
    Missing SELECT clause components & \texttt{SELECT DISTINCT flight\_1.flight\_id FROM flight flight\_1, airport\_service airport\_service\_1, city cit...} & Queries often have truncated or incomplete SELECT/FROM clauses, missing proper table aliases or column specifications. & ~200/466 (approximately 40-50\% of queries) \\
    \bottomrule
  \end{tabular}
  \label{tab:qualitative}
  \caption{Use this table for your qualitative analysis on the dev set.}\label{tab:qualitative}
\end{table}
\end{landscape}

\section*{Q7.}

Provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted. 
\textcolor{gray}{TODO}

\section*{Extra Credit: }

If you are doing extra credit assignment, please describe your system here, as well as provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted. 
\textcolor{gray}{Optional TODO}
\end{document}