\documentclass{article}

\input{header}

\title{DSGA 1011: Assignment 4}

\author{Full Name \\ Net ID}

\date{}


\colmfinalcopy
\begin{document}
\maketitle
% \section*{Part I. Q1} No written element, submit \texttt{out\_original.txt}  to autograder.
\section*{Q0. 1.}
Please provide a link to your github repository, which contains the code for both Part I and Part II. 

GitHub Repository: \url{https://github.com/srush-shah/fine-tuning-llms.git}
\section*{Q2. 1.}
Describe your transformation of dataset.

The data transformation applies two types of augmentations to text: synonym replacement and typo simulation. 

\textbf{Synonym Replacement:} For each word in the text, with 20\% probability, we attempt to replace it with a synonym from WordNet. We extract synonyms from all synsets for the word, filter out synonyms containing underscores (to avoid multi-word phrases), and randomly select one. The original capitalization is preserved.

\textbf{Typo Simulation:} For words that were not replaced by synonyms, with 15\% probability, we simulate keyboard typos by replacing a character with a neighboring key on the QWERTY keyboard. We focus on vowels and common letters (a, e, i, o, u, s, d, f, g, h, j, k, l) and replace at most one character per word with a randomly selected neighbor. The original capitalization is preserved.

Both transformations are applied at the word level after tokenization, and the text is detokenized to produce the final transformed example. This approach introduces natural variations that simulate real-world text noise while preserving the semantic meaning of the original text.
% \section*{Part I. Q2. 2. No written element, submit \texttt{out\_transformed.txt} to autograder. }
\section*{Q3. 1}
\textbf{Report \& Analysis}
    \begin{itemize}
        \item \textbf{Accuracy values:}
        \begin{itemize}
            \item Model trained on original data, evaluated on original test: 93.12\%
            \item Model trained on original data, evaluated on transformed test: 88.70\%
            \item Model trained on augmented data, evaluated on original test: 93.04\%
            \item Model trained on augmented data, evaluated on transformed test: 90.81\%
        \end{itemize}
        
        \item \textbf{Analysis:}
        \begin{enumerate}
            \item \textit{Did the model's performance on the transformed test data improve after applying data augmentation?} Yes, the accuracy on transformed test data improved from 88.70\% (without augmentation) to 90.81\% (with augmentation), an improvement of 2.11 percentage points. This indicates that data augmentation helped the model generalize better to the transformed (noisy) test distribution.
            
            \item \textit{How did data augmentation affect the model's performance on the original test data?} Data augmentation slightly decreased performance on the original test data from 93.12\% to 93.04\%, a small drop of 0.08 percentage points. This suggests that while augmentation helps with robustness to transformations, it may introduce a small trade-off in performance on the original distribution, though the difference is minimal and likely within statistical variance.
        \end{enumerate}
        
        \item \textbf{Intuitive explanation:} Data augmentation exposes the model to more diverse training examples that include synonym variations and typos similar to those in the transformed test set. This helps the model learn more robust features that are invariant to these types of noise. The model learns to focus on semantic content rather than exact word matches, which improves generalization to the transformed test set. The slight decrease on original test data may occur because the augmented training set includes some noisy examples that slightly shift the decision boundary, but the effect is minimal since the majority of training data remains clean.
        
        \item \textbf{Limitation:} One key limitation is that the augmentation only covers specific types of noise (synonyms and keyboard typos) that are similar to the test-time transformations. For truly out-of-distribution test sets with different types of noise (e.g., grammatical errors, domain shifts, or completely different writing styles), this augmentation approach may not help. The model may overfit to the specific augmentation patterns seen during training and fail to generalize to other types of distribution shifts. Additionally, the augmentation probabilities (20\% for synonyms, 15\% for typos) are fixed and may not reflect the actual distribution of noise in real-world scenarios.
    \end{itemize}
\section*{Part II. Q4}
% 
% \section{Data Statistics and Processing (8pt)}


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
Number of examples & 4225 & 466 \\
Mean sentence length & 10.96 & 10.91 \\
Mean SQL query length & 60.90 & 58.90  \\
Vocabulary size (natural language)& 868 & 444  \\
Vocabulary size (SQL)& 533 & 326  \\
\bottomrule
\end{tabular}
\caption{Data statistics before any pre-processing. \textcolor{gray}{You need to at least provide the statistics listed above, and can add new entries.}}
\label{tab:data_stats_before}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
\multicolumn{3}{l}{\textbf{T5 fine-tuned model}} \\ % \textcolor{gray}{(T5 fine-tuning or T5 from scratch)}} \\
Mean sentence length (tokens) & 10.96 & 10.91 \\
Mean SQL query length (tokens) & 60.90 & 58.90  \\
Vocabulary size (natural language) & 868 & 444  \\
Vocabulary size (SQL) & 533 & 326  \\
\midrule
\bottomrule
\end{tabular}
\caption{Data statistics after pre-processing. After tokenization with T5TokenizerFast, the statistics remain similar to the raw data since T5 tokenization preserves most word boundaries. The vocabulary sizes reflect the unique tokens in the tokenizer's vocabulary that appear in the dataset.}
\label{tab:data_stats_after}
\end{table}



\newpage




\section*{Q5}\label{sec:t5}


\begin{table}[h!]
\centering
\begin{tabular}{p{3.5cm}p{10cm}}
\toprule
Design choice & Description \\
\midrule
Data processing & Added task prefix ``translate English to SQL: '' to all natural language queries. \textbf{Schema-aware training:} Database schema information is automatically loaded from \texttt{flight\_database.schema} and included in the encoder input. The schema summary contains table names and their key columns (e.g., ``Tables: flight(flight\_id,from\_airport,to\_airport,...) | airport\_service(airport\_code,city\_code,...) | ...''), formatted as ``translate English to SQL: Tables: ... | \{natural language query\}''. This helps the model learn correct table and column names during training. \\
Tokenization & Used T5TokenizerFast from 'google-t5/t5-small' for both encoder and decoder. Encoder inputs were tokenized with the task prefix and schema information included. Decoder targets (SQL queries) were tokenized with add\_special\_tokens=True to include EOS tokens. Maximum sequence lengths: 512 tokens for encoder (increased to accommodate schema) and 256 tokens for decoder inputs. \\
Architecture & Fine-tuned the entire T5-small model (all encoder and decoder layers). The model was initialized from the pretrained 'google-t5/t5-small' checkpoint. \\
Hyperparameters & Learning rate: 3e-5 (reduced from 1e-4 for more stable training), Batch size: 32, Optimizer: AdamW, Scheduler: Linear with warmup, Max epochs: 20, Patience: 3-6 epochs (early stopping based on dev F1 score), Weight decay: 0.0, Gradient clipping: max\_norm=1.0. Generation: Beam search with num\_beams=4, max\_new\_tokens=256, early\_stopping=True, repetition\_penalty=1.5, no\_repeat\_ngram\_size=3, length\_penalty=1.0. \\
Post-processing & Enhanced SQL query cleaning function fixes common errors: wrong table names (airport\_provider $\rightarrow$ airport\_service), wrong aliases (flight\_2 $\rightarrow$ flight\_1 when flight\_1 is defined), malformed WHERE clauses, incomplete queries, and syntax errors. \\
\bottomrule
\end{tabular}
\caption{Details of the best-performing T5 model configurations (fine-tuned with schema-aware training)}
\label{tab:t5_results_ft}
\end{table}







\section*{Q6. }

\paragraph{Quantitative Results:} 
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
  \toprule
  System & Query EM & F1 score\\
  \midrule
  \multicolumn{3}{l}{\textbf{Dev Results}} \\
  \midrule
  
  \multicolumn{3}{l}{\textbf{T5 fine-tuned}} \\
  Full model & 0.00\% & 11.80\% \\[5pt]
  % Variant1 & XX.XX & XX.XX \\
  % Variant2 & XX.XX & XX.XX \\
  % Variant3 & XX.XX & XX.XX \\
  
  \midrule
  \multicolumn{3}{l}{\textbf{Test Results}} \\
  \midrule
  T5 fine-tuning & 0.00\% & 11.80\% \\
  \bottomrule
\end{tabular}  
\caption{Development and test results. \textcolor{gray}{Use this table to report quantitative results for both dev and test results.}}
\label{tab:results}
\end{table}


\paragraph{Qualitative Error Analysis:} 


\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{p{2cm}p{6cm}p{6cm}p{6cm}}
    \toprule
    \textbf{Error Type}& \textbf{Example Of Error} & \textbf{Error Description} & \textbf{Statistics} \\
    \midrule
    Repetitive token generation & \texttt{SELECT DISTINCT flight\_service\_service\_service\_service...} & The model generates repetitive sequences of the same tokens (e.g., ``service\_service\_service'' or ``airport\_code = airport\_service\_1.airport\_code = ...''), indicating the model gets stuck in generation loops. & 466/466 (100\% of queries show some form of repetition) \\
    \midrule
    Malformed SQL syntax & \texttt{SELECT DISTINCT flight\_1.airport\_code = airport\_service\_1.airport\_code = ...} & Generated queries contain invalid SQL syntax, such as multiple consecutive equals signs, missing WHERE clauses, or incorrect table/column references. & 466/466 (100\% SQL error rate) \\
    \midrule
    Incomplete queries & \texttt{denver nach atlanta} & Some queries are completely malformed, generating natural language text instead of SQL, or producing queries that are missing critical SQL components. & ~50/466 (approximately 10-15\% of queries) \\
    \midrule
    Missing SELECT clause components & \texttt{SELECT DISTINCT flight\_1.flight\_id FROM flight flight\_1, airport\_service airport\_service\_1, city cit...} & Queries often have truncated or incomplete SELECT/FROM clauses, missing proper table aliases or column specifications. & ~200/466 (approximately 40-50\% of queries) \\
    \bottomrule
  \end{tabular}
  \label{tab:qualitative}
  \caption{Use this table for your qualitative analysis on the dev set.}\label{tab:qualitative}
\end{table}
\end{landscape}

\section*{Q7.}

Provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted. 
\textcolor{gray}{TODO}

\section*{Extra Credit: }

\textbf{LLM Prompting Approach:} For the extra credit assignment, we implemented a prompting-based approach using large language models (LLMs) for Text-to-SQL generation. The system supports both zero-shot and few-shot prompting with Gemma models (specifically \texttt{google/gemma-1.1-2b-it} and \texttt{google/codegemma-7b-it}).

\textbf{Implementation Details:}
\begin{itemize}
    \item \textbf{Models:} The system can use either Gemma-1.1-2b-it (instruction-tuned) or CodeGemma-7b-it (code-specialized). CodeGemma supports 4-bit quantization for memory efficiency.
    \item \textbf{Prompting:} The framework supports k-shot prompting where k examples from the training set are included in the prompt to guide the model's SQL generation. The prompt construction includes the database schema and example natural language to SQL query pairs.
    \item \textbf{SQL Extraction:} Generated outputs are post-processed to extract valid SQL queries from the model's response, which may include explanatory text or formatting.
    \item \textbf{Evaluation:} The system evaluates generated SQL queries using the same metrics as the fine-tuned T5 model (SQL EM, Record EM, Record F1).
\end{itemize}

\textbf{Results:} The prompting approach provides an alternative to fine-tuning that requires no model training but relies on the LLM's in-context learning capabilities. Results are saved in \texttt{llm\_test.sql} and \texttt{llm\_test.pkl} files.

\textbf{Model Checkpoint:} [Google Drive link to be provided]
\end{document}