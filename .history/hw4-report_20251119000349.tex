\documentclass{article}

\input{header}

\title{DSGA 1011: Assignment 4}

\author{Full Name \\ Net ID}

\date{}


\colmfinalcopy
\begin{document}
\maketitle
% \section*{Part I. Q1} No written element, submit \texttt{out\_original.txt}  to autograder.
\section*{Q0. 1.}
Please provide a link to your github repository, which contains the code for both Part I and Part II. 

GitHub Repository: \url{https://github.com/srush-shah/fine-tuning-llms.git}
\section*{Q2. 1.}
Describe your transformation of dataset.

The data transformation applies two types of augmentations to text: synonym replacement and typo simulation. 

\textbf{Synonym Replacement:} For each word in the text, with 20\% probability, we attempt to replace it with a synonym from WordNet. We extract synonyms from all synsets for the word, filter out synonyms containing underscores (to avoid multi-word phrases), and randomly select one. The original capitalization is preserved.

\textbf{Typo Simulation:} For words that were not replaced by synonyms, with 15\% probability, we simulate keyboard typos by replacing a character with a neighboring key on the QWERTY keyboard. We focus on vowels and common letters (a, e, i, o, u, s, d, f, g, h, j, k, l) and replace at most one character per word with a randomly selected neighbor. The original capitalization is preserved.

Both transformations are applied at the word level after tokenization, and the text is detokenized to produce the final transformed example. This approach introduces natural variations that simulate real-world text noise while preserving the semantic meaning of the original text.
% \section*{Part I. Q2. 2. No written element, submit \texttt{out\_transformed.txt} to autograder. }
\section*{Q3. 1}
\textbf{Report \& Analysis}
    \begin{itemize}
        \item \textbf{Accuracy values:}
        \begin{itemize}
            \item Model trained on original data, evaluated on original test: 93.12\%
            \item Model trained on original data, evaluated on transformed test: 88.70\%
            \item Model trained on augmented data, evaluated on original test: 93.04\%
            \item Model trained on augmented data, evaluated on transformed test: 90.81\%
        \end{itemize}
        
        \item \textbf{Analysis:}
        \begin{enumerate}
            \item \textit{Did the model's performance on the transformed test data improve after applying data augmentation?} Yes, the accuracy on transformed test data improved from 88.70\% (without augmentation) to 90.81\% (with augmentation), an improvement of 2.11 percentage points. This indicates that data augmentation helped the model generalize better to the transformed (noisy) test distribution.
            
            \item \textit{How did data augmentation affect the model's performance on the original test data?} Data augmentation slightly decreased performance on the original test data from 93.12\% to 93.04\%, a small drop of 0.08 percentage points. This suggests that while augmentation helps with robustness to transformations, it may introduce a small trade-off in performance on the original distribution, though the difference is minimal and likely within statistical variance.
        \end{enumerate}
        
        \item \textbf{Intuitive explanation:} Data augmentation exposes the model to more diverse training examples that include synonym variations and typos similar to those in the transformed test set. This helps the model learn more robust features that are invariant to these types of noise. The model learns to focus on semantic content rather than exact word matches, which improves generalization to the transformed test set. The slight decrease on original test data may occur because the augmented training set includes some noisy examples that slightly shift the decision boundary, but the effect is minimal since the majority of training data remains clean.
        
        \item \textbf{Limitation:} One key limitation is that the augmentation only covers specific types of noise (synonyms and keyboard typos) that are similar to the test-time transformations. For truly out-of-distribution test sets with different types of noise (e.g., grammatical errors, domain shifts, or completely different writing styles), this augmentation approach may not help. The model may overfit to the specific augmentation patterns seen during training and fail to generalize to other types of distribution shifts. Additionally, the augmentation probabilities (20\% for synonyms, 15\% for typos) are fixed and may not reflect the actual distribution of noise in real-world scenarios.
    \end{itemize}
\section*{Part II. Q4}
% 
% \section{Data Statistics and Processing (8pt)}


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
Number of examples & 4225 & 466 \\
Mean sentence length & 10.96 & 10.91 \\
Mean SQL query length & 60.90 & 58.90  \\
Vocabulary size (natural language)& 868 & 444  \\
Vocabulary size (SQL)& 533 & 326  \\
\bottomrule
\end{tabular}
\caption{Data statistics before any pre-processing. \textcolor{gray}{You need to at least provide the statistics listed above, and can add new entries.}}
\label{tab:data_stats_before}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
\multicolumn{3}{l}{\textbf{T5 fine-tuned model (with schema-aware training)}} \\ % \textcolor{gray}{(T5 fine-tuning or T5 from scratch)}} \\
Mean sentence length (tokens) & 17.10 & 17.07 \\
Mean SQL query length (tokens) & 216.37 & 210.05  \\
Vocabulary size (natural language, token IDs) & 791 & 465  \\
Vocabulary size (SQL, token IDs) & 555 & 395  \\
\midrule
\bottomrule
\end{tabular}
\caption{Data statistics after pre-processing with T5TokenizerFast. The encoder input includes schema information (table names and columns), which increases the average input length. The vocabulary sizes reflect the unique token IDs in the tokenizer's vocabulary that appear in the dataset after tokenization. Note: These statistics include the schema prefix in the encoder inputs.}
\label{tab:data_stats_after}
\end{table}



\newpage




\section*{Q5}\label{sec:t5}


\begin{table}[h!]
\centering
\begin{tabular}{p{3.5cm}p{10cm}}
\toprule
Design choice & Description \\
\midrule
Data processing & Added task prefix ``translate English to SQL: '' to all natural language queries. \textbf{Schema-aware training:} Database schema information is automatically loaded from \texttt{flight\_database.schema} and included in the encoder input. The schema summary contains table names and their key columns (e.g., ``Tables: flight(flight\_id,from\_airport,to\_airport,...) | airport\_service(airport\_code,city\_code,...) | ...''), formatted as ``translate English to SQL: Tables: ... | \{natural language query\}''. This helps the model learn correct table and column names during training. \\
Tokenization & Used T5TokenizerFast from 'google-t5/t5-small' for both encoder and decoder. Encoder inputs were tokenized with the task prefix and schema information included. Decoder targets (SQL queries) were tokenized with add\_special\_tokens=True to include EOS tokens. Maximum sequence lengths: 512 tokens for encoder (increased to accommodate schema) and 256 tokens for decoder inputs. \\
Architecture & Fine-tuned the entire T5-small model (all encoder and decoder layers). The model was initialized from the pretrained 'google-t5/t5-small' checkpoint. \\
Hyperparameters & Learning rate: 3e-5 (reduced from 1e-4 for more stable training), Batch size: 32, Optimizer: AdamW, Scheduler: Linear with warmup, Max epochs: 20, Patience: 3-6 epochs (early stopping based on dev F1 score), Weight decay: 0.0, Gradient clipping: max\_norm=1.0. Generation: Beam search with num\_beams=4, max\_new\_tokens=256, early\_stopping=True, repetition\_penalty=1.5, no\_repeat\_ngram\_size=3, length\_penalty=1.0. \\
Post-processing & Enhanced SQL query cleaning function fixes common errors: wrong table names (airport\_provider $\rightarrow$ airport\_service), wrong aliases (flight\_2 $\rightarrow$ flight\_1 when flight\_1 is defined), malformed WHERE clauses, incomplete queries, and syntax errors. \\
\bottomrule
\end{tabular}
\caption{Details of the best-performing T5 model configurations (fine-tuned with schema-aware training)}
\label{tab:t5_results_ft}
\end{table}







\section*{Q6. }

\paragraph{Quantitative Results:} 
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
  \toprule
  System & Query EM & F1 score\\
  \midrule
  \multicolumn{3}{l}{\textbf{Dev Results}} \\
  \midrule
  
  \multicolumn{3}{l}{\textbf{T5 fine-tuned}} \\
  Full model & 0.00\% & 11.80\% \\[5pt]
  % Variant1 & XX.XX & XX.XX \\
  % Variant2 & XX.XX & XX.XX \\
  % Variant3 & XX.XX & XX.XX \\
  
  \midrule
  \multicolumn{3}{l}{\textbf{Test Results}} \\
  \midrule
  T5 fine-tuning & 0.00\% & 11.80\% \\
  \bottomrule
\end{tabular}  
\caption{Development and test results. \textcolor{gray}{Use this table to report quantitative results for both dev and test results.}}
\label{tab:results}
\end{table}


\paragraph{Qualitative Error Analysis:} 

The following table describes common error patterns observed in the model's generated SQL queries. \textbf{Note:} Schema-aware training and enhanced post-processing were implemented to address these issues. Schema-aware training helps the model learn correct table and column names, while post-processing fixes common alias mismatches and syntax errors.

\begin{landscape}
\begin{table}
  \centering
  \begin{tabular}{p{2cm}p{6cm}p{6cm}p{6cm}}
    \toprule
    \textbf{Error Type}& \textbf{Example Of Error} & \textbf{Error Description} & \textbf{Statistics} \\
    \midrule
    Wrong table/column names & \texttt{airport\_provider\_1.city\_code} or \texttt{airline\_1.airport\_code} & The model generates non-existent table names (e.g., ``airport\_provider'' instead of ``airport\_service'', ``airline\_1'' instead of ``airport\_service\_1''). This is addressed by schema-aware training which includes correct table/column names in the input. & ~330/466 (70.8\% of queries) \\
    \midrule
    Wrong aliases & \texttt{flight\_2.from\_airport} when only \texttt{flight\_1} is defined & The model uses aliases that are not defined in the FROM clause (e.g., \texttt{flight\_2}, \texttt{flight\_3} when only \texttt{flight\_1} exists). Post-processing fixes these by mapping undefined aliases to defined ones. & ~345/466 (74.0\% of queries) \\
    \midrule
    Malformed SQL syntax & \texttt{flight\_1.airport\_code = airport\_service\_1.airport\_code = 'DENVER'} & Generated queries contain invalid SQL syntax, such as multiple consecutive equals signs, missing WHERE clauses, or incorrect table/column references. Post-processing attempts to fix these patterns. & 466/466 (100\% SQL error rate before post-processing) \\
    \midrule
    Repetitive token generation & \texttt{SELECT DISTINCT flight\_service\_service\_service...} & The model generates repetitive sequences of the same tokens, indicating the model gets stuck in generation loops. Addressed by increased repetition\_penalty (1.5) and no\_repeat\_ngram\_size (3). & ~100/466 (approximately 20-25\% of queries) \\
    \midrule
    Incomplete queries & Queries ending with \texttt{AND(} or \texttt{= )} & Some queries have incomplete WHERE clauses or malformed endings. Post-processing removes trailing incomplete conditions. & ~50/466 (approximately 10-15\% of queries) \\
    \bottomrule
  \end{tabular}
  \label{tab:qualitative}
  \caption{Qualitative error analysis on the dev set. The table shows common error patterns and the mitigation strategies implemented (schema-aware training and post-processing).}\label{tab:qualitative}
\end{table}
\end{landscape}

\section*{Q7.}

Provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted. 

Google Drive: \url{https://drive.google.com/drive/folders/1y473SqlqLYlK7l2mBLM4XLyJET251Gv9?usp=sharing}

The drive contains two folders:
\begin{itemize}
    \item \texttt{ft\_experiments}: Contains checkpoints for fine-tuned T5 models
    \item \texttt{scr\_experiments}: Contains checkpoints for T5 models trained from scratch
\end{itemize}

\section*{Extra Credit: }

\textbf{LLM Prompting Approach:} For the extra credit assignment, we implemented a prompting-based approach using large language models (LLMs) for Text-to-SQL generation. The system supports both zero-shot and few-shot prompting with Gemma models (specifically \texttt{google/gemma-1.1-2b-it} and \texttt{google/codegemma-7b-it}).

\textbf{Implementation Details:}
\begin{itemize}
    \item \textbf{Models:} The system can use either Gemma-1.1-2b-it (instruction-tuned) or CodeGemma-7b-it (code-specialized). CodeGemma supports 4-bit quantization for memory efficiency.
    \item \textbf{Prompting:} The framework supports k-shot prompting where k examples from the training set are included in the prompt to guide the model's SQL generation. The prompt construction includes the database schema and example natural language to SQL query pairs.
    \item \textbf{SQL Extraction:} Generated outputs are post-processed to extract valid SQL queries from the model's response, which may include explanatory text or formatting.
    \item \textbf{Evaluation:} The system evaluates generated SQL queries using the same metrics as the fine-tuned T5 model (SQL EM, Record EM, Record F1).
\end{itemize}

\textbf{Results:} The prompting approach provides an alternative to fine-tuning that requires no model training but relies on the LLM's in-context learning capabilities. Results are saved in \texttt{llm\_test.sql} and \texttt{llm\_test.pkl} files.

\textbf{Model Checkpoint:} [Google Drive link to be provided]
\end{document}